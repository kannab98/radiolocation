% Тип документа
\documentclass[a4paper,14pt]{extarticle}
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}
\usepackage[T2A]{fontenc}
\usepackage
    { % Дополнения Американского математического общества (AMS)
        amssymb,
        amsfonts,
        amsmath,
        amsthm,
        % Пакет для физических текстов
        physics,
        color,
        float,
        ulem,
        esint,
        esdiff,
        % 
    } 
    
\usepackage{mathtools}
\mathtoolsset{showonlyrefs=true} 

\usepackage{xcolor}
\usepackage{hyperref}
 % Цвета для гиперссылок
\definecolor{linkcolor}{HTML}{000000} % цвет ссылок
\definecolor{urlcolor}{HTML}{799B03} % цвет гиперссылок
 
\hypersetup{linkcolor=linkcolor,urlcolor=urlcolor,colorlinks=true}
\hypersetup{citecolor=linkcolor}
\hypersetup{pageanchor=false}

% Увеличенный межстрочный интервал, французские пробелы
\linespread{1.3} 
\frenchspacing 

\newcommand{\mean}[1]{\langle#1\rangle}
\newcommand*{\const}{\mathrm{const}}
\renewcommand*{\arctg}{arctg}
%\renewcommand*{\kappa}{\kappa}
\renewcommand*{\phi}{\varphi}

\newcommand{\tK}{\widetilde K}
%\renewcommand{\qty}{ }


\begin{document}


%\section{Метод наименьших квадратов Прони}%
%\label{sec:metod_naimen_shikh_kvadratov_proni}
%\subsection{Матрица ковариации}%
%\label{sub:matritsa_kovariatsii}

%\begin{equation}
    %\label{eq:Rxx1}
    %R_{xx} = 
        %\begin{bmatrix}
            %r_{xx}[0] & r_{xx}^*[1] & \dots &  r_{xx}^*[n] \\
            %r_{xx}[1] & r_{xx}[0] & \dots &  r_{xx}^*[n-1] \\
            %\dots  & \dots    & \dots & \dots   \\ 
            %r_{xx}[p] & r_{xx}[p-1] & \dots &  r_{xx}^*[n-p] \\
        %\end{bmatrix}
%\end{equation}
%Можно обойтись без комплексного сопряжения, если учесть, что функция ковариации
%обладает следующим свойством $r_{xx}[-p] = r_{xx}^*[p]$.

%\begin{equation}
    %\label{eq:Rxx2}
    %R_{xx} = 
        %\begin{bmatrix}
            %r_{xx}[0] & r_{xx}[1] & \dots &  r_{xx}[-n] \\
            %r_{xx}[1] & r_{xx}[0] & \dots &  r_{xx}[-n+1] \\
            %\dots  & \dots    & \dots & \dots   \\ 
            %r_{xx}[p] & r_{xx}[p-1] & \dots &  r_{xx}[-n+p] \\
        %\end{bmatrix}
%\end{equation}

%Корреляция $r_{xx}$ вычисляется следующим образом, если индексс отсчитывать от
%единицы:
%\begin{equation}
    %\label{eq:rxx1}
    %r_{xx}[p] = \frac{1}{n-p+1} \sum\limits_{j=1}^{n-p} x[j+p] \cdot x^*[j]
%\end{equation}
%Или, если применять индексы $p<0$:
%\begin{equation}
    %\label{eq:rxx2}
    %r_{xx}[p] = \frac{1}{n - |p| + 1} \sum\limits_{j=1}^{n-|p|} x^*[j+|p|] \cdot x[j]
%\end{equation}

%Зная автокорреляционную матрицу, можно найти коэффициент авторегрессии, решая
%уравненния Юла-Уокера (6.32) из книги Марпл.
%\begin{equation}
    %\label{eq:Rxx2}
        %\begin{pmatrix}
            %r_{xx}[0] & r_{xx}[-1] & \dots &  r_{xx}[-n] \\
            %r_{xx}[1] & r_{xx}[0] & \dots &  r_{xx}[-n+1] \\
            %\dots  & \dots    & \dots & \dots   \\ 
            %r_{xx}[p] & r_{xx}[p-1] & \dots &  r_{xx}[-n+p] \\
        %\end{pmatrix}
        %\cdot 
        %\begin{pmatrix}
            %1 \\
            %a_p[1]\\
            %\vdots\\
            %a_p[n]
        %\end{pmatrix}
        %=
        %\begin{pmatrix}
            %\rho_0 \\
            %0\\
            %\vdots\\
            %0
        %\end{pmatrix}
%\end{equation}

%\subsection{Оценка ошибки предсказания}%
%\label{sub:otsenka_oshibki_predskazaniia}


%Коэффициенты авторегрессии нужны для нахождения коэффициентов отражения $k_p$:
%\begin{equation}
    %\label{eq:kp}
    %k_p = - \frac{
                %\sum\limits_{n=1}^{\infty} a_{p-1}[n] r_{xx}[p-n]
        %}{
                %\rho_{p-1}
        %}, \text{ где}
%\end{equation}

%дисперсия $\rho_p$ связана рекурсивным соотношением с $\rho_0 = r_{xx}[0]$
%\begin{equation}
    %\label{eq:}
    %\rho_p = \rho_{p-1} (1 - \abs{k_p}^2)
%\end{equation}

%Теперь можем найти ошибку предсказания вперед и назад
%\begin{align}
    %\label{eq:err-front}
    %e_p^f[n]  =  e_{p-1}^f[n]    + k_p e_{p-1}^b [n-1]\\
    %\label{eq:err-back}
    %e_p^b[n]  =  e_{p-1}^b[n-1]  + k_p^* e_{p-1}^f [n]\\
%\end{align}


%В свою очередь, ошибка предсказания необходима для 



%\section{ЕЩЁ РАЗ}%
%\label{sec:eshchio_raz}

%В случае переопределенных данных
%\begin{equation}
    %\begin{pmatrix}
        %x_1 \\
        %x_2 \\
        %\vdots \\
        %x_{N}
    %\end{pmatrix}
    %=
    %\begin{pmatrix}
        %z_1^0 & z_2^0 & \dots & z_p^0 \\ 
        %z_1^1 & z_2^1 & \dots & z_p^1 \\ 
        %\dots & \dots & \dots & \dots \\
        %z_1^{N-1} & z_2^{N-1} & \dots & z_p^{N-1} \\ 
        
    %\end{pmatrix}
    %\cdot 
    %\begin{pmatrix}
        %h_1 \\ 
        %h_2 \\ 
        %\dots \\ 
        %h_p \\ 
    %\end{pmatrix}
%\end{equation}



\section{Идея метода}%
\label{sec:ideia_metoda}


Метод Прони заключается в аппроксимации последовательности данных $x[n]$
моделью $\tilde x[n]$,
состоящей из $p$ затухающих комплексных экспонент
\begin{equation}
    \tilde x[n] = \sum\limits_{k=1}^{p} A_k\exp{\alpha_k Tn} \exp{j2\pi f_k Tn
    +j\phi_k}, \text{ где } 1\leq n\leq N,~ 1\leq p \leq \frac{N}{2}
\end{equation}

Представим это выражение в cокращенном виде
\begin{equation}
    \label{eq:}
    \tilde x[n] = \sum\limits_{k=1}^{p} h_k z_{k}^{n-1}, \text{ где}
\end{equation}
    $h_k = A_k \exp(j \phi_k)$ -- комплексная амплитуда k-ой экспоненты,
    $z_k= \exp(\alpha_kT + j 2\pif_kT)$ -- k-ая комплексная экспонента. 


Или в матричном виде
\begin{equation}
    \label{eq:tildex}
    \begin{pmatrix}
        x_1 \\ 
        x_2 \\ 
        \dots \\ 
        x_N \\ 
    \end{pmatrix}
    =
    \begin{pmatrix}
        z_1^0 & z_2^0 & \dots & z_p^0 \\ 
        z_1^1 & z_2^1 & \dots & z_p^1 \\ 
        \dots & \dots & \dots & \dots \\
        z_1^{N-1} & z_2^{N-1} & \dots & z_p^{N-1} \\ 
    \end{pmatrix}
    \cdot 
    \begin{pmatrix}
        h_1 \\ 
        h_2 \\ 
        \dots \\ 
        h_p \\ 
    \end{pmatrix}
\end{equation}

Если мы сможем найти метод для раздельного нахождения элементов $z$, то мы
сможем решить уравнение \eqref{eq:tildex} как обычную систему уранений
относительно неизвестных переменных $h$.  



\section{Теоретическая часть}%
\label{sec:teoreticheskaia_chast_}


Составим матрицу размерности $p \times N$, где первый индекс число строк, а
второй -- число столбцов
 \begin{equation}
    \label{eq:a}
    \hat a = 
    \begin{pmatrix}
        a_p & a_{p-1} & \dots & a_0 & 0 & \dots & 0 \\
        0  & a_{p} & \dots & a_1 & a_0 & \dots & 0 \\
        \dots & \dots & \dots & \dots & \dots & \dots & \dots\\
        0  & \dots &  0  & a_p & \dots  & a_1 & a_{0} \\
    \end{pmatrix}
\end{equation}

Теперь умножим уравнение \eqref{eq:tildex} на $\hat a$ слева.
Тогда левая часть уравнения \eqref{eq:tildex} следующий примет вид
\begin{equation}
    \begin{pmatrix}
        a_p & a_{p-1} & \dots & a_0 & 0 & \dots & 0 \\
        0  & a_{p} & \dots & a_1 & a_0 & \dots & 0 \\
        \dots & \dots & \dots & \dots & \dots & \dots & \dots\\
        0  & \dots &  0  & a_p & \dots  & a_1 & a_{0} \\
    \end{pmatrix}
    \cdot 
    \begin{pmatrix}
        x_1 \\ 
        x_2 \\ 
        \dots \\ 
        x_N \\ 
    \end{pmatrix}
    =
    \begin{pmatrix}
        \sum\limits_{m=0}^{p} a_m x_{p+1-m} \\ 
        \sum\limits_{m=0}^{p} a_m x_{p+2-m} \\ 
        \dots \\ 
        \sum\limits_{m=0}^{p} a_m x_{N-m} \\ 
    \end{pmatrix}
\end{equation}

А правая часть уравнения \eqref{eq:tildex} 
\begin{equation}
    (p,N) \cdot (N,p) = (p,p) 
\end{equation}
\begin{multline}
    \begin{pmatrix}
        a_p & a_{p-1} & \dots & a_0 & 0 & \dots & 0 \\
        0  & a_{p} & \dots & a_1 & a_0 & \dots & 0 \\
        \dots & \dots & \dots & \dots & \dots & \dots & \dots\\
        0  & \dots &  0  & a_p & \dots  & a_1 & a_{0} \\
    \end{pmatrix}
    \cdot 
    \begin{pmatrix}
        z_1^0 & z_2^0 & \dots & z_p^0 \\ 
        z_1^1 & z_2^1 & \dots & z_p^1 \\ 
        \dots & \dots & \dots & \dots \\
        z_1^{N-1} & z_2^{N-1} & \dots & z_p^{N-1} \\ 
    \end{pmatrix}
    = \\
    \begin{pmatrix}
        \sum\limits_{m=0}^{p} a_m z_1^{p-m}&  \sum\limits_{m=0}^{p} a_m z_2^{p-m}& \dots & \sum\limits_{m=0}^{p} a_m z_p^{p-m}   \\ 
        z_1\sum\limits_{m=0}^{p} a_m z_1^{p-m}&  z_2\sum\limits_{m=0}^{p} a_m z_2^{p-m}& \dots & z_p\sum\limits_{m=0}^{p} a_m z_p^{p-m}   \\ 
        \dots & \dots & \dots & \dots \\
        z_1^{N-p-1}\sum\limits_{m=0}^{p} a_m z_1^{p-m}&
        z_2^{N-m-1}\sum\limits_{m=0}^{p} a_m z_2^{p-m}& \dots &
        z_p^{N-m-1}\sum\limits_{m=0}^{p} a_m z_p^{p-m}   \\ 
    \end{pmatrix}
\end{multline}

Поскольку мы вводили коэффициенты $a_m$ произвольно, теперь уточним их. Пусть
коэффициенты  $a_m$ обеспечивают выполнение равенства 
\begin{equation}
    \label{eq:poly}
    \sum\limits_{m=0}^{p} a_m z_k^{p-m} = 0, \text{ для } k = 1,2,\dots,p
\end{equation}
при этом $a_0=1$.

Тогда правая часть уравнения \eqref{eq:tildex} при умножении на матрицу $\hat
a$ занулится. Остается
 \begin{equation}
    \label{eq:ax}
    \begin{pmatrix}
        \sum\limits_{m=0}^{p} a_m x_{p+1-m} \\ 
        \sum\limits_{m=0}^{p} a_m x_{p+2-m} \\ 
        \dots \\ 
        \sum\limits_{m=0}^{p} a_m x_{N-m} \\ 
    \end{pmatrix}
    = 0 
\end{equation}

То есть мы получили систему уравнений из $N-p$ уравнений.

\paragraph{Немного о смысле уравнения \eqref{eq:ax}.}
Запишем уравнение \eqref{eq:ax} через разложение в экспоненциальный ряд
$\sum\limits_{k=1}^{p} h_k z_k^{n-1}$

 \begin{equation}
    \begin{pmatrix}
        \sum\limits_{m=0}^{p} a_m x_{p+1-m} \\ 
        \sum\limits_{m=0}^{p} a_m x_{p+2-m} \\ 
        \dots \\ 
        \sum\limits_{m=0}^{p} a_m x_{N-m} \\ 
    \end{pmatrix}
    =
    \begin{pmatrix}
        \sum\limits_{m=0}^{p} a_m \sum\limits_{k=1}^{p} h_k z_k^{p-m} \\ 
        \sum\limits_{m=0}^{p} a_m \sum\limits_{k=1}^{p} h_k z_k^{p-m+1}\\ 
        \dots \\ 
        \sum\limits_{m=0}^{p}  a_m \sum\limits_{k=1}^{p} h_k z_k^{N-m -1}\\ 
    \end{pmatrix}
\end{equation}
Поменяем местами суммы
\begin{equation}
    \label{eq:}
    \begin{pmatrix}
        \sum\limits_{m=0}^{p} a_m \sum\limits_{k=1}^{p} h_k z_k^{p-m} \\ 
        \sum\limits_{m=0}^{p} a_m \sum\limits_{k=1}^{p} h_k z_k^{p-m+1}\\ 
        \dots \\ 
        \sum\limits_{m=0}^{p}  a_m \sum\limits_{k=1}^{p} h_k z_k^{N-m -1}\\ 
    \end{pmatrix}
    =
    \begin{pmatrix}
        \sum\limits_{k=1}^{p} h_k \sum\limits_{m=0}^{p} a_m  z_k^{p-m} \\ 
        \sum\limits_{k=1}^{p} h_k z_m\sum\limits_{m=0}^{p} a_m  z_k^{p-m} \\ 
        \dots \\ 
        \sum\limits_{k=1}^{p} h_k z_k^{N-p-1}\sum\limits_{m=0}^{p} a_m  z_k^{p-m} \\ 
    \end{pmatrix}
\end{equation}
Сумму в правой части можно рассматривать, как некий полином записанный через
свой корни, что и обеспечивает тождественное нулю равенство.

Именно для этого мы вводили особым образом матрицу $\hat a$ и уравнение
\eqref{eq:poly}. 

Но при аппроксимации последовательности $x_n$ последовательностью  $\tilde x$,
мы не сможем точно определить корни полинома  \eqref{eq:poly}, у нас будет
некоторая погрешность.

Это подводит нас к тому, что уравнение \eqref{eq:ax} должно быть
модифицировано 

 \begin{equation}
    \label{eq:axe}
    \begin{pmatrix}
        \sum\limits_{m=0}^{p} a_m x_{p+1-m} \\ 
        \sum\limits_{m=0}^{p} a_m x_{p+2-m} \\ 
        \dots \\ 
        \sum\limits_{m=0}^{p} a_m x_{N-m} \\ 
    \end{pmatrix}
    = 
    \begin{pmatrix}
        e_1 \\
        e_2 \\
        \dots \\
        e_{N-p} \\
    \end{pmatrix}, \text{ где}
\end{equation}
$e$ -- характеризует ошибку \underline{линейной} аппроксимации, хотя
изначально вы вводили ошибку $\epsilon$ как ошибку экспоненциальной
аппроксимации.

Теперь будем минимизировать квадрат ошибок функции \eqref{eq:axe}
\begin{equation}
    \label{eq:rho}
    \rho = \sum\limits_{n=1}^{N-p} e_n e_{n}^* = 
    \sum\limits_{n=1}^{N-p}
    \sum\limits_{m=0}^{p} a_m x_{p+n-m} 
    \sum\limits_{k=0}^{p} a_k^* x^*_{p+n-k} 
    \to \text{ min}
\end{equation}
{ \color{red} Пусть $a_m = a_m^*$.Можно ли так делать? }

Чтобы \eqref{eq:rho} выполнялось необходимо продифференцировать каждое
слагаемое $\rho$ по всем
$a_m$ и найти их минимум, т.е.

\begin{multline}
    \pdv{\rho}{a_0} = 
        \sum\limits_{n=1}^{N-p} x_{n+p}x^*_{p+n}  +
        a_1 \sum\limits_{n=1}^{N-p} x_{p+n-1}x^*_{p+n}  +
        a_2\sum\limits_{n=1}^{N-p} x_{p+n-2}x^*_{p+n} + \hdots 
        \\
        + a_p\sum\limits_{n=1}^{N-p} x_{n}x^*_{p+n} = 0
\end{multline}

\begin{multline}
    \pdv{\rho}{a_1} = 
        \sum\limits_{n=1}^{N-p} x_{n+p}x^*_{p+n-1}  +
        a_1 \sum\limits_{n=1}^{N-p} x_{p+n-1}x^*_{p+n-1}  +
        a_2\sum\limits_{n=1}^{N-p} x_{p+n-2}x^*_{p+n-1} + \hdots 
        \\
        + a_p\sum\limits_{n=1}^{N-p} x_{n}x^*_{p+n-1} = 0
\end{multline}

\begin{multline}
    \pdv{\rho}{a_2} = 
        \sum\limits_{n=1}^{N-p} x_{n+p}x^*_{p+n-2}  +
        a_1 \sum\limits_{n=1}^{N-p} x_{p+n-1}x^*_{p+n-2}  +
        a_2\sum\limits_{n=1}^{N-p} x_{p+n-2}x^*_{p+n-2} + \hdots  
        \\
        + a_p\sum\limits_{n=1}^{N-p} x_{n}x^*_{p+n-2} = 0
\end{multline}

Вспомним, что конструкция вида $\frac{1}{N-m} \sum\limits_{n=0}^{N-m-1} x_{n+m}
x_n^*$ есть выражение для дискретно-временной оценки автокорреляции
$r_{xx}[m]$.

Приведем суммы в выражениях 

Составим на основе получившихся производных матрицу ковариации
\begin{equation}
    \label{eq:}
    \hat R_{xx} = 
    \begin{pmatrix}
        r_{xx}[0] & r_{xx}^*[1] & r_{xx}^*[2] & \dots & r_{xx}^*[p] \\        
        r_{xx}[1] & r_{xx}[0] & r_{xx}^*[1] & \dots & r_{xx}^*[p] \\        
        r_{xx}[2] & r_{xx}[1] & r_{xx}[0] & \dots & r_{xx}^*[p] \\        
        \dots & \dots & \dots & \dots & \dots \\
        r_{xx}[p] & r_{xx}[p-1] & r_{xx}[-2] & \dots & r_{xx}[0] \\        
    \end{pmatrix}
\end{equation}

Получили систему уравений
\begin{equation}
    \label{eq:solve:a}
    \boxed{
    \hat R_{xx} \cdot \hat a = 0}, \text{ где }
\end{equation}
вектор-столбец  $\hat a=      
    \begin{pmatrix}
        1 \\
        a_1 \\
        \vdots \\
        a_p \\
    \end{pmatrix} $ 
-- неизвестнен.

Найденные из уравнения \eqref{eq:solve:a} коэффициенты $a$ подставим в
уравнение  \eqref{eq:poly}. Получаем замкнутую систему уравнений для $p$
неизвестных  $z_k$.
\begin{equation}
    \sum\limits_{m=0}^{p} a_m z_k^{p-m} = 0, \text{ для } k = 1,2,\dots,p
\end{equation}



Вспомним теперь, что реальная последовательность $x_n$ отличается от
аппроксимированной последовательности  $\tilde x_n$ на величину ошибки
экспоненциальной аппроксимации  $\epsilon$
\begin{equation}
    \label{eq:}
    x_n = \tilde x_n + \epsilon_n
\end{equation}
Из этих соображений, модифицируем уравнение \eqref{eq:tildex} 

\begin{equation}
    \label{eq:x}
    \begin{pmatrix}
        z_1^0 & z_2^0 & \dots & z_p^0 \\ 
        z_1^1 & z_2^1 & \dots & z_p^1 \\ 
        \dots & \dots & \dots & \dots \\
        z_1^{N-1} & z_2^{N-1} & \dots & z_p^{N-1} \\ 
    \end{pmatrix}
    \cdot 
    \begin{pmatrix}
        h_1 \\ 
        h_2 \\ 
        \dots \\ 
        h_p \\ 
    \end{pmatrix}
    -
    \begin{pmatrix}
        x_1 \\ 
        x_2 \\ 
        \dots \\ 
        x_N \\ 
    \end{pmatrix}
    =
    \begin{pmatrix}
        \epsilon_1 \\ 
        \epsilon_2 \\ 
        \dots \\ 
        \epsilon_N \\ 
    \end{pmatrix}
\end{equation}

Теперь вновь нужно минимизировать cумму квадратом ошибок
\begin{equation}
    \label{eq:}
    \rho = \sum\limits_{n=1}^{N} \epsilon_n \epsilon_n^* \to \text{min} 
\end{equation}

Вновь дифференцируя  по параметру $h_p$ получаем систему уравнений
вида 
 \begin{equation}
    \label{eq:}
    \boxed{\hat Z \cdot \hat h = \hat C}, \text{ где}
\end{equation}
\begin{equation}
    \label{eq:}
    \hat Z = 
    \begin{pmatrix}
        \sum\limits_{n=1}^{N} z_1^{n-1} (z_1^*)^{n-1} &
        \sum\limits_{n=1}^{N} z_2^{n-1} (z_1^*)^{n-1} &
        \dots &
        \sum\limits_{n=1}^{N} z_p^{n-1} (z_1^*)^{n-1} &
        \\
        \sum\limits_{n=1}^{N} z_1^{n-1} (z_2^*)^{n-1} &
        \sum\limits_{n=1}^{N} z_2^{n-2} (z_2^*)^{n-1} &
        \dots &
        \sum\limits_{n=1}^{N} z_p^{n-1} (z_2^*)^{n-1} &
        \\
        \dots & \dots & \dots & \dots &
        \\
        \sum\limits_{n=1}^{N} z_1^{n-1} (z_p^*)^{n-1} &
        \sum\limits_{n=1}^{N} z_2^{n-2} (z_p^*)^{n-1} &
        \dots &
        \sum\limits_{n=1}^{N} z_p^{n-1} (z_p^*)^{n-1} &
    \end{pmatrix},
\end{equation}
\begin{equation}
    \label{eq:}
    \hat h = 
    \begin{pmatrix}
        h_1 \\ 
        h_2 \\ 
        \dots \\ 
        h_p \\ 
    \end{pmatrix},
\end{equation}

\begin{equation}
    \label{eq:}
    \hat C = 
    \begin{pmatrix}
        \sum\limits_{n=1}^{N} x_n (z_1^*)^{n-1} \\ 
        \sum\limits_{n=1}^{N} x_n (z_2^*)^{n-1} \\ 
        \dots \\
        \sum\limits_{n=1}^{N} x_n (z_p^*)^{n-1} \\ 
    \end{pmatrix},
\end{equation}

Из этой системы мы найдем вектор-столбец $\hat h$.

\section{Краткая схема}%
\label{sec:kratkaia_skhema}


\label{par:itog}


\begin{enumerate}
    \item Составляем матрицу ковариации (см. \eqref{eq:solve:a})  
    \item Находим множители полинома $a_m$  (см. \eqref{eq:solve:a})  
    \item Находим затухающие экспоненты $z_k$ (cм. \eqref{eq:poly} )
    \item Находим комплексные амплитуды экспонент $h_k$ 
    \item Находим все остальные параметры модели
        \begin{gather}
            f_k = \frac{1}{2 \pi T} \arctan{\frac{\Im{z_k}}{\Re{z_k}}} \\
            \alpha_k = \frac{1}{T} \ln\abs{z_k} \\
            A_k = \abs{h_k}\\
            \phi_k =  \arctan{\frac{\Im{h_k}}{\Re{h_k}}} \\
        \end{gather}
\end{enumerate}







%Если верить Марплу, то уравнение \eqref{eq:axe} также характеризует ошибку
%линейного предсказания  




\end{document}
